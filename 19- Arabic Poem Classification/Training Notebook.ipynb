{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7797293,"sourceId":71335,"sourceType":"competition"}],"dockerImageVersionId":30646,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP DL Pipeline (Training)","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nplt.rcParams[\"figure.figsize\"] = (12, 8)\nfrom tqdm.auto import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, StratifiedGroupKFold\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n# os.system('pip install -q bitsandbytes')\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nfrom torch.optim import Optimizer\nimport torch.nn.init as init\nimport torch.nn.functional as F\nfrom torch.autograd.function import InplaceFunction\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\n# PyTorch Lightning imports:\nimport pytorch_lightning as pl\nfrom pytorch_lightning import LightningModule, Trainer, seed_everything\nfrom pytorch_lightning.tuner.tuning import Tuner\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping \nfrom pytorch_lightning.loggers import CSVLogger\nfrom torchmetrics.classification import MulticlassAccuracy\n\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=False\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"tags":[]},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"tokenizers.__version__: 0.12.1\n\ntransformers.__version__: 4.21.3\n\nenv: TOKENIZERS_PARALLELISM=False\n"}]},{"cell_type":"markdown","source":"# Pipeline","metadata":{}},{"cell_type":"markdown","source":"## Directory Settings","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n    \nTRAIN_PATH = 'arabic-poem-classification/poems.csv'\nTEST_PATH = 'arabic-poem-classification/test.csv'","metadata":{"tags":[]},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Data Loading","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Data Loading\n# ====================================================\ntrain_df = pd.read_csv(TRAIN_PATH)\ntest_df = pd.read_csv(TEST_PATH)\n\nprint(f\"train.shape: {train_df.shape}\")\ndisplay(train_df.head())","metadata":{"tags":[]},"execution_count":23,"outputs":[{"name":"stdout","output_type":"stream","text":"train.shape: (25000, 4)\n"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Title</th>\n","      <th>Author</th>\n","      <th>Category</th>\n","      <th>Poem</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>هُنّئتَ بالعيدِ بل هُنّي بكَ العيدُ،</td>\n","      <td>صفي الدين الحلي</td>\n","      <td>العصر الأندلسي</td>\n","      <td>هنءت بالعيد بل هني بك العيد فانت لجود بل ارث ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>يا بَديعَ الحُسنِ</td>\n","      <td>المعتمد بن عباد</td>\n","      <td>العصر الأندلسي</td>\n","      <td>يا بديع الحسن والاح سان يا بدر الدياجي يا غزا...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>وأوانس تدنو إذا اجتديت</td>\n","      <td>الأبيوردي</td>\n","      <td>العصر الأندلسي</td>\n","      <td>وَأَوانِسٍ تَدنو إِذا اِجتَدِيَتْ بِحَديثها وَ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>حسبي رضاك من الدهر الذي عتبا</td>\n","      <td>ابن دارج القسطلي</td>\n","      <td>العصر الأندلسي</td>\n","      <td>حسبي رضاك من الدهر الذي عتبا وجود كفيك لحظ ال...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ماذا يقول المادحو</td>\n","      <td>ظافر الحداد</td>\n","      <td>العصر الأندلسي</td>\n","      <td>ماذا يقول المادحو  نَ وأنت مُخترع الغَرائبْ أع...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                   Title            Author        Category                                               Poem\n","0  هُنّئتَ بالعيدِ بل هُنّي بكَ العيدُ،    صفي الدين الحلي  العصر الأندلسي   هنءت بالعيد بل هني بك العيد فانت لجود بل ارث ...\n","1                     يا بَديعَ الحُسنِ    المعتمد بن عباد  العصر الأندلسي   يا بديع الحسن والاح سان يا بدر الدياجي يا غزا...\n","2                 وأوانس تدنو إذا اجتديت         الأبيوردي  العصر الأندلسي  وَأَوانِسٍ تَدنو إِذا اِجتَدِيَتْ بِحَديثها وَ...\n","3          حسبي رضاك من الدهر الذي عتبا   ابن دارج القسطلي  العصر الأندلسي   حسبي رضاك من الدهر الذي عتبا وجود كفيك لحظ ال...\n","4                      ماذا يقول المادحو       ظافر الحداد  العصر الأندلسي  ماذا يقول المادحو  نَ وأنت مُخترع الغَرائبْ أع..."]},"metadata":{}}]},{"cell_type":"code","source":"train_df['Poem'] = \"العنوان \" + train_df['Title'] + \" والكاتب هو \" + train_df['Author'] + \" :والنص هو \" + train_df['Poem']","metadata":{"tags":[]},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Drop Duplicates\nprint('Samples Before Dropping: ', train_df.shape[0])\ntrain_df = train_df.drop_duplicates(subset=['Poem']).reset_index(drop=True)\nprint('Samples After Dropping: ', train_df.shape[0])","metadata":{"tags":[]},"execution_count":31,"outputs":[{"name":"stdout","output_type":"stream","text":"Samples Before Dropping:  25000\n\nSamples After Dropping:  20412\n"}]},{"cell_type":"code","source":"feats = ['Category']\nfor feat in feats: \n    Names = [f'{feat}_{x}' for x in train_df[feat].value_counts().keys().sort_values()]\n    OHE_cols = pd.DataFrame(pd.get_dummies(train_df[feat]).values,index = train_df.index, columns = Names)\n    train_df = pd.concat([train_df,OHE_cols],axis=1)\ntrain_df.columns[-5:].values","metadata":{"tags":[]},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":["array(['Category_العصر الأندلسي', 'Category_العصر الايوبي',\n","       'Category_العصر العباسي', 'Category_العصر المملوكي',\n","       'Category_العصر حديث'], dtype=object)"]},"metadata":{}}]},{"cell_type":"code","source":"le = LabelEncoder()\ndf = pd.concat([train_df, test_df])\nle.fit(df[\"Category\"])\n\nmapping = dict(zip(le.classes_, range(len((le.classes_)))))\nmapping_rev = dict(zip(range(len((le.classes_))), le.classes_))\n\ntrain_df['Category'] = train_df['Category'].transform(lambda x: mapping[x])","metadata":{"tags":[]},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## CFG","metadata":{}},{"cell_type":"code","source":"class CFG:\n    competition = 'Poem'   # Competition Name\n    seed = 42\n########################################################################################################\n    # Status\n    train = True          # Train Mode\n    debug = False         # Debug Mode \n########################################################################################################\n    # Data\n    max_len = 256        \n    batch_size = 16        \n    valid_batch_size = 16\n    num_workers = os.cpu_count()       # Threads in Data Loader\n    target_cols = ['Category_العصر الأندلسي', 'Category_العصر الايوبي', 'Category_العصر العباسي', 'Category_العصر المملوكي', 'Category_العصر حديث']\n########################################################################################################\n    # Training\n    model = 'CAMeL-Lab/bert-base-arabic-camelbert-ca'\n    pretrained = True\n    epochs = 5\n    \n    max_grad_norm = 10  # Prevent Gradient Explosion by Clipping Gradinet when exceeding this number\n    freeze_n_layers = 0              # Freeze First n_layers of the Encoder\n    layer_reinitialize_n = 1         # Reinitialize the last n layers of the encoder\n \n    pooling = 'mean'                  \n    features_type = 'weighted_layers_cls'  \n    output_hidden_states = True     \n    layer_start = 9                     \n########################################################################################################\n    # Optimizer\n    encoder_lr = 1.5e-5         # Pretrained Model lr  (Deberta Model)\n    decoder_lr = 1.5e-5         # Custom Model lr  (The new head of the model)\n    \n    eps = 1e-6                # Adam Parameters \n    betas=(0.9, 0.999)        # Adam Parameters\n    weight_decay = 0.02\n    \n    precision = \"16-mixed\"\n########################################################################################################\n    # Scheduler\n    use_scheduler = True    # Use Scheduler\n    scheduler = 'cosine'      # 'cosine' or 'linear' or 'cosine_hard'\n    num_cycles = 0.25\n    num_warmup_steps = 1\n    sch_interval = 'step'    # 'step' or 'epoch'\n########################################################################################################\n    # CV\n    n_fold=5\n    trn_fold=list(range(n_fold))","metadata":{"tags":[]},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"if CFG.debug:\n    CFG.epochs = 2\n    CFG.trn_fold = [0]","metadata":{"tags":[]},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_preds, y_trues):\n    metric = MulticlassAccuracy(num_classes=5).to(device)\n    score = metric(y_preds, y_trues)\n    return score\n\nseed_everything(seed=CFG.seed)","metadata":{"tags":[]},"execution_count":55,"outputs":[{"name":"stderr","output_type":"stream","text":"Seed set to 42\n"},{"execution_count":55,"output_type":"execute_result","data":{"text/plain":["42"]},"metadata":{}}]},{"cell_type":"markdown","source":"## CV Split","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CV split\n# ====================================================\nFold = StratifiedGroupKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\nfor n, (train_index, val_index) in enumerate(Fold.split(train_df, train_df['Category'],groups=train_df['Author'])):\n    train_df.loc[val_index, 'fold'] = int(n)\ntrain_df['fold'] = train_df['fold'].astype(int)\ndisplay(train_df.groupby('fold').size())","metadata":{"tags":[]},"execution_count":56,"outputs":[{"output_type":"display_data","data":{"text/plain":["fold\n","0    3371\n","1    4219\n","2    4746\n","3    4807\n","4    3269\n","dtype: int64"]},"metadata":{}}]},{"cell_type":"code","source":"if CFG.debug:\n    display(train_df.groupby('fold').size())\n    train_df = train_df.sample(n=100, random_state=CFG.seed).reset_index(drop=True)\n    display(train_df.groupby('fold').size())","metadata":{"tags":[]},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\ntokenizer = AutoTokenizer.from_pretrained(CFG.model, use_fast=True)\ntokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\nCFG.tokenizer = tokenizer","metadata":{"tags":[]},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Define max_len\n# ====================================================\nlengths = []\ntk0 = tqdm(train_df['Poem'].fillna(\"\").values, total=len(train_df))\nfor text in tk0:\n    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n    lengths.append(length)\nCFG.max_len = max(lengths) + 2 # cls & sep \nprint(f\"max_len: {CFG.max_len}\")","metadata":{"tags":[]},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45e573e82f6942eb87c64b24d5ee86ba","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/20409 [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"max_len: 20017\n"}]},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\n# Split the sentences into tokens & Make the sentences have fixed length (padding for short - truncating for long)\ndef prepare_input(cfg, text):\n    inputs = cfg.tokenizer.encode_plus(\n        text, \n        return_tensors=None, \n        add_special_tokens=True, \n        max_length=256,\n        pad_to_max_length=True,\n        truncation=True\n    )\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TrainDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['Poem'].values\n        self.labels = df[cfg.target_cols].values\n        self.tabular_df = df[feats].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        label = torch.tensor(self.labels[item], dtype=torch.float)\n        tabular = torch.tensor(self.tabular_df[item], dtype=torch.float)\n        return {'input_ids' : inputs['input_ids'], \n                'attention_mask' : inputs['attention_mask'], \n                'labels' : label}\n    \ndef collate(inputs):\n    # Dynamic padding (pad the inputs of the batch to the maximum input length of the batch.)\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n    for k, v in inputs.items():\n        inputs[k] = inputs[k][:,:mask_len]\n    return inputs","metadata":{"tags":[]},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"#### Last Hidden State Poolings","metadata":{}},{"cell_type":"code","source":"class MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\nclass WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_hidden_states):\n        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weight_factor = weight_factor.to(device)\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n        return weighted_average","metadata":{"tags":[]},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"#### Get optimizer and scheduler","metadata":{}},{"cell_type":"code","source":"def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': weight_decay},\n            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': 0.0},\n            {'params': [p for n, p in model.named_parameters() if \"fc\" in n],\n             'lr': decoder_lr, 'weight_decay': 0.0}\n        ]\n              \n        return optimizer_parameters\n    \n    \ndef get_scheduler(cfg, optimizer, len_train_folds):\n    num_train_steps = int(len_train_folds / cfg.batch_size * cfg.epochs)\n    if cfg.scheduler == 'linear':\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n        )\n    elif cfg.scheduler == 'cosine':\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n        )\n    elif cfg.scheduler == 'cosine_hard':\n        scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n        )\n        \n    return scheduler","metadata":{"tags":[]},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"#### The Model","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel(LightningModule):\n    def __init__(self, cfg, criterion, train_folds, valid_folds, fold, pretrained=False, config_path=None):\n        super().__init__()\n        self.cfg = cfg\n        self.criterion = criterion\n        self.train_folds = train_folds\n        self.valid_folds = valid_folds\n        self.fold = fold\n        self.pretrained = pretrained\n        self.val_step_outputs = []\n        self.val_step_labels = []\n\n        \n        # Configurations\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states = True)\n        else:\n            self.config = torch.load(config_path)\n        \n        # Model\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n            \n        torch.save(self.model.config, OUTPUT_DIR+'config.pth')\n\n        self.pool = MeanPooling()\n        self.fc = nn.Linear(self.config.hidden_size, len(cfg.target_cols))\n        \n        # initalize the header\n        self._init_weights(self.fc)\n                \n        # Reinitialize the last n layers      \n        self._re_init_layers(self.cfg.layer_reinitialize_n)\n            \n    def _init_weights(self, module: nn.Module):\n            if isinstance(module, nn.Linear):\n                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                if module.bias is not None:\n                    module.bias.data.zero_()\n                return \"nn.Linear\"\n            elif isinstance(module, nn.Embedding):\n                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                if module.padding_idx is not None:\n                    module.weight.data[module.padding_idx].zero_()\n                return \"nn.Embedding\"\n            elif isinstance(module, nn.LayerNorm):\n                module.bias.data.zero_()\n                module.weight.data.fill_(1.0)\n                return \"nn.LayerNorm\"\n            return None\n    \n    def _re_init_layers(self, n_layers: int):\n            \"\"\"Reinitialize the last n layers \"\"\"\n            if n_layers >= 1:\n                for layer in self.model.encoder.layer[-n_layers:]:\n                    # Confirmed that it works with deberta v3. Other models may be different.\n                    if hasattr(layer, \"modules\"):\n                        for module in layer.modules():\n                            for name, child in module.named_children():\n                                init_type_name = self._init_weights(child)\n                                if init_type_name is not None:\n                                    print(f\"{name} is re-initialized, type: {init_type_name}, {module.__class__}\")\n            \n    def train_dataloader(self):\n        train_dataset = TrainDataset(CFG, self.train_folds)\n        train_loader = DataLoader(train_dataset,\n                                  batch_size=CFG.batch_size,\n                                  shuffle=True,\n                                  num_workers=CFG.num_workers,\n                                  drop_last=True)\n        return train_loader\n    \n    \n    def val_dataloader(self):\n        valid_dataset = TrainDataset(CFG, self.valid_folds)\n        valid_loader = DataLoader(valid_dataset,\n                                  batch_size=CFG.valid_batch_size,\n                                  shuffle=False,\n                                  num_workers=CFG.num_workers,\n                                  drop_last=False)\n        return valid_loader\n    \n    def training_step(self, batch, batch_idx):\n        inputs, labels = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}, batch['labels']\n        inputs = collate(inputs)\n        batch_size = labels.size(0)\n\n        y_preds = self(inputs)\n        loss = self.criterion(y_preds, labels)\n\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n        for param_group in self.trainer.optimizers[0].param_groups:\n            lr = param_group[\"lr\"]\n        self.log(\"lr\", lr, on_step=True, on_epoch=False, prog_bar=True)\n\n        return loss\n\n    \n    def validation_step(self, batch, batch_idx):\n        inputs, labels = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}, batch['labels']\n        inputs = collate(inputs)\n        batch_size = labels.size(0)\n\n        y_preds = self(inputs)\n        loss = self.criterion(y_preds, labels)\n\n        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n\n        self.val_step_outputs.append(y_preds)\n        self.val_step_labels.append(labels)\n\n        return loss\n\n        \n    def configure_optimizers(self):\n        optimizer_parameters = get_optimizer_params(self.model,\n                                                    encoder_lr=CFG.encoder_lr,\n                                                    decoder_lr=CFG.decoder_lr,\n                                                    weight_decay=CFG.weight_decay)\n        \n        optimizer = AdamW(optimizer_parameters,\n                              lr=CFG.encoder_lr,\n                              eps=CFG.eps,\n                              betas=CFG.betas)\n        \n        scheduler = get_scheduler(CFG, optimizer, len(self.train_folds))\n        lr_scheduler_dict = {\"scheduler\": scheduler, \"interval\": CFG.sch_interval}\n        if self.cfg.use_scheduler:\n            return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler_dict}\n        else:\n            return {'optimizer': optimizer}\n    \n    def on_validation_epoch_end(self):\n        all_preds = torch.cat(self.val_step_outputs)\n        all_labels = torch.cat(self.val_step_labels)\n        self.val_step_outputs.clear()\n        self.val_step_labels.clear()\n        \n        all_preds = nn.Softmax(dim=1)(all_preds)\n        score = get_score(all_preds, all_labels)\n        self.log(\"accuracy_score\", score, on_step=False, on_epoch=True, prog_bar=True)\n        if self.trainer.global_rank == 0:\n            print(f\"\\nEpoch: {self.current_epoch}, accuracy_score: {score}\", flush=True)\n           \n    \n    # Return hidden states \n    def feature(self, input_ids, attention_mask):\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        # Weighted Layers CLS\n        all_hidden_states = torch.stack(outputs.hidden_states)\n        pooler = WeightedLayerPooling(\n            self.config.num_hidden_layers, \n            layer_start=self.cfg.layer_start, layer_weights=None\n        )\n        weighted_pooling_embeddings = pooler(all_hidden_states)\n        feature = weighted_pooling_embeddings[:, 0]\n       \n        return feature\n    \n    # The Model Architicture\n    def forward(self, batch):\n        feature = self.feature(batch['input_ids'], batch['attention_mask'])\n        output = self.fc(feature)\n        return output","metadata":{"tags":[]},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"## Training Loop","metadata":{}},{"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss()\nfor fold in CFG.trn_fold:\n    print(f\"Fold: {fold}\")\n    train_folds = train_df.loc[train_df['fold'] != fold].reset_index(drop=True)\n    valid_folds = train_df.loc[train_df['fold'] == fold].reset_index(drop=True)\n    \n    model = CustomModel(cfg=CFG,\n                        criterion=criterion,\n                        train_folds=train_folds,\n                        valid_folds=valid_folds,\n                        fold=fold,\n                        pretrained=True)\n\n    checkpoint_callback = ModelCheckpoint(\n        save_weights_only=True,\n        monitor=\"accuracy_score\",\n        dirpath=OUTPUT_DIR,\n        mode=\"max\",\n        filename=f\"model-f{fold}-{{accuracy_score:.4f}}\",\n        save_top_k=1,\n        verbose=1,\n    )\n\n\n    trainer = Trainer(max_epochs=CFG.epochs,devices=1,\n                      accelerator='gpu', deterministic=False, \n                      precision=CFG.precision, strategy='auto',\n                      callbacks=[checkpoint_callback],\n                      logger=CSVLogger(save_dir=f'logs_f{fold}/'),)\n    trainer.fit(model)","metadata":{"tags":[]},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"Fold: 0\n"},{"name":"stderr","output_type":"stream","text":"Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-ca were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\nUsing 16bit Automatic Mixed Precision (AMP)\n\nGPU available: True (cuda), used: True\n\nTPU available: False, using: 0 TPU cores\n\nIPU available: False, using: 0 IPUs\n\nHPU available: False, using: 0 HPUs\n\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"},{"name":"stdout","output_type":"stream","text":"query is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertSelfAttention'>\n\nkey is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertSelfAttention'>\n\nvalue is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertSelfAttention'>\n\ndense is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n\nLayerNorm is re-initialized, type: nn.LayerNorm, <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n\ndense is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertIntermediate'>\n\ndense is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertOutput'>\n\nLayerNorm is re-initialized, type: nn.LayerNorm, <class 'transformers.models.bert.modeling_bert.BertOutput'>\n"},{"name":"stderr","output_type":"stream","text":"\n\n  | Name              | Type              | Params\n\n--------------------------------------------------------\n\n0 | criterion         | BCEWithLogitsLoss | 0     \n\n1 | tabular_processor | Sequential        | 168   \n\n2 | model             | BertModel         | 109 M \n\n3 | pool              | MeanPooling       | 0     \n\n4 | fc                | Linear            | 3.8 K \n\n--------------------------------------------------------\n\n85.6 M    Trainable params\n\n23.4 M    Non-trainable params\n\n109 M     Total params\n\n436.341   Total estimated model params size (MB)\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 0, accuracy_score: 0.5\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7eef335512f34d48b4404935aaf97933","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 0, accuracy_score: 0.7587882280349731\n"},{"name":"stderr","output_type":"stream","text":"Epoch 0, global step 1065: 'accuracy_score' reached 0.75879 (best 0.75879), saving model to '/notebooks/model-f0-accuracy_score=0.7588.ckpt' as top 1\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 1, accuracy_score: 0.7894912362098694\n"},{"name":"stderr","output_type":"stream","text":"Epoch 1, global step 2130: 'accuracy_score' reached 0.78949 (best 0.78949), saving model to '/notebooks/model-f0-accuracy_score=0.7895.ckpt' as top 1\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 2, accuracy_score: 0.8309106826782227\n"},{"name":"stderr","output_type":"stream","text":"Epoch 2, global step 3195: 'accuracy_score' reached 0.83091 (best 0.83091), saving model to '/notebooks/model-f0-accuracy_score=0.8309.ckpt' as top 1\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 3, accuracy_score: 0.8448160886764526\n"},{"name":"stderr","output_type":"stream","text":"Epoch 3, global step 4260: 'accuracy_score' reached 0.84482 (best 0.84482), saving model to '/notebooks/model-f0-accuracy_score=0.8448.ckpt' as top 1\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 4, accuracy_score: 0.8416271209716797\n"},{"name":"stderr","output_type":"stream","text":"Epoch 4, global step 5325: 'accuracy_score' was not in top 1\n\n`Trainer.fit` stopped: `max_epochs=5` reached.\n"},{"name":"stdout","output_type":"stream","text":"Fold: 1\n"},{"name":"stderr","output_type":"stream","text":"Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-ca were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\nUsing 16bit Automatic Mixed Precision (AMP)\n\nGPU available: True (cuda), used: True\n\nTPU available: False, using: 0 TPU cores\n\nIPU available: False, using: 0 IPUs\n\nHPU available: False, using: 0 HPUs\n\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n  | Name              | Type              | Params\n\n--------------------------------------------------------\n\n0 | criterion         | BCEWithLogitsLoss | 0     \n\n1 | tabular_processor | Sequential        | 168   \n\n2 | model             | BertModel         | 109 M \n\n3 | pool              | MeanPooling       | 0     \n\n4 | fc                | Linear            | 3.8 K \n\n--------------------------------------------------------\n\n85.6 M    Trainable params\n\n23.4 M    Non-trainable params\n\n109 M     Total params\n\n436.341   Total estimated model params size (MB)\n"},{"name":"stdout","output_type":"stream","text":"query is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertSelfAttention'>\n\nkey is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertSelfAttention'>\n\nvalue is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertSelfAttention'>\n\ndense is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n\nLayerNorm is re-initialized, type: nn.LayerNorm, <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n\ndense is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertIntermediate'>\n\ndense is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertOutput'>\n\nLayerNorm is re-initialized, type: nn.LayerNorm, <class 'transformers.models.bert.modeling_bert.BertOutput'>\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 0, accuracy_score: 0.5\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"79474ce7cd6e484db79b18d60d08ad16","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 0, accuracy_score: 0.6232519149780273\n"},{"name":"stderr","output_type":"stream","text":"Epoch 0, global step 1012: 'accuracy_score' reached 0.62325 (best 0.62325), saving model to '/notebooks/model-f1-accuracy_score=0.6233.ckpt' as top 1\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 1, accuracy_score: 0.6296515464782715\n"},{"name":"stderr","output_type":"stream","text":"Epoch 1, global step 2024: 'accuracy_score' reached 0.62965 (best 0.62965), saving model to '/notebooks/model-f1-accuracy_score=0.6297.ckpt' as top 1\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 2, accuracy_score: 0.672374963760376\n"},{"name":"stderr","output_type":"stream","text":"Epoch 2, global step 3036: 'accuracy_score' reached 0.67237 (best 0.67237), saving model to '/notebooks/model-f1-accuracy_score=0.6724.ckpt' as top 1\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 3, accuracy_score: 0.6772635579109192\n"},{"name":"stderr","output_type":"stream","text":"Epoch 3, global step 4048: 'accuracy_score' reached 0.67726 (best 0.67726), saving model to '/notebooks/model-f1-accuracy_score=0.6773.ckpt' as top 1\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 4, accuracy_score: 0.6474282741546631\n"},{"name":"stderr","output_type":"stream","text":"Epoch 4, global step 5060: 'accuracy_score' was not in top 1\n\n`Trainer.fit` stopped: `max_epochs=5` reached.\n"},{"name":"stdout","output_type":"stream","text":"Fold: 2\n"},{"name":"stderr","output_type":"stream","text":"Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-ca were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\nUsing 16bit Automatic Mixed Precision (AMP)\n\nGPU available: True (cuda), used: True\n\nTPU available: False, using: 0 TPU cores\n\nIPU available: False, using: 0 IPUs\n\nHPU available: False, using: 0 HPUs\n\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n  | Name              | Type              | Params\n\n--------------------------------------------------------\n\n0 | criterion         | BCEWithLogitsLoss | 0     \n\n1 | tabular_processor | Sequential        | 168   \n\n2 | model             | BertModel         | 109 M \n\n3 | pool              | MeanPooling       | 0     \n\n4 | fc                | Linear            | 3.8 K \n\n--------------------------------------------------------\n\n85.6 M    Trainable params\n\n23.4 M    Non-trainable params\n\n109 M     Total params\n\n436.341   Total estimated model params size (MB)\n"},{"name":"stdout","output_type":"stream","text":"query is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertSelfAttention'>\n\nkey is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertSelfAttention'>\n\nvalue is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertSelfAttention'>\n\ndense is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n\nLayerNorm is re-initialized, type: nn.LayerNorm, <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n\ndense is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertIntermediate'>\n\ndense is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertOutput'>\n\nLayerNorm is re-initialized, type: nn.LayerNorm, <class 'transformers.models.bert.modeling_bert.BertOutput'>\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 0, accuracy_score: 0.5\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f07e5972ade04476a17d605b218b0eee","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 0, accuracy_score: 0.5864412188529968\n"},{"name":"stderr","output_type":"stream","text":"Epoch 0, global step 979: 'accuracy_score' reached 0.58644 (best 0.58644), saving model to '/notebooks/model-f2-accuracy_score=0.5864.ckpt' as top 1\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 1, accuracy_score: 0.6420406699180603\n"},{"name":"stderr","output_type":"stream","text":"Epoch 1, global step 1958: 'accuracy_score' reached 0.64204 (best 0.64204), saving model to '/notebooks/model-f2-accuracy_score=0.6420.ckpt' as top 1\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 2, accuracy_score: 0.6030604839324951\n"},{"name":"stderr","output_type":"stream","text":"Epoch 2, global step 2937: 'accuracy_score' was not in top 1\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 3, accuracy_score: 0.6332437992095947\n"},{"name":"stderr","output_type":"stream","text":"Epoch 3, global step 3916: 'accuracy_score' was not in top 1\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 4, accuracy_score: 0.6351137757301331\n"},{"name":"stderr","output_type":"stream","text":"Epoch 4, global step 4895: 'accuracy_score' was not in top 1\n\n`Trainer.fit` stopped: `max_epochs=5` reached.\n"},{"name":"stdout","output_type":"stream","text":"Fold: 3\n"},{"name":"stderr","output_type":"stream","text":"Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-ca were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\nUsing 16bit Automatic Mixed Precision (AMP)\n\nGPU available: True (cuda), used: True\n\nTPU available: False, using: 0 TPU cores\n\nIPU available: False, using: 0 IPUs\n\nHPU available: False, using: 0 HPUs\n\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n  | Name              | Type              | Params\n\n--------------------------------------------------------\n\n0 | criterion         | BCEWithLogitsLoss | 0     \n\n1 | tabular_processor | Sequential        | 168   \n\n2 | model             | BertModel         | 109 M \n\n3 | pool              | MeanPooling       | 0     \n\n4 | fc                | Linear            | 3.8 K \n\n--------------------------------------------------------\n\n85.6 M    Trainable params\n\n23.4 M    Non-trainable params\n\n109 M     Total params\n\n436.341   Total estimated model params size (MB)\n"},{"name":"stdout","output_type":"stream","text":"query is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertSelfAttention'>\n\nkey is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertSelfAttention'>\n\nvalue is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertSelfAttention'>\n\ndense is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n\nLayerNorm is re-initialized, type: nn.LayerNorm, <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n\ndense is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertIntermediate'>\n\ndense is re-initialized, type: nn.Linear, <class 'transformers.models.bert.modeling_bert.BertOutput'>\n\nLayerNorm is re-initialized, type: nn.LayerNorm, <class 'transformers.models.bert.modeling_bert.BertOutput'>\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 0, accuracy_score: 0.5\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"24a0ea2476d84677b7c73123963791fb","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 0, accuracy_score: 0.646713137626648\n"},{"name":"stderr","output_type":"stream","text":"Epoch 0, global step 975: 'accuracy_score' reached 0.64671 (best 0.64671), saving model to '/notebooks/model-f3-accuracy_score=0.6467.ckpt' as top 1\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 1, accuracy_score: 0.6617953181266785\n"},{"name":"stderr","output_type":"stream","text":"Epoch 1, global step 1950: 'accuracy_score' reached 0.66180 (best 0.66180), saving model to '/notebooks/model-f3-accuracy_score=0.6618.ckpt' as top 1\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 2, accuracy_score: 0.6448408365249634\n"},{"name":"stderr","output_type":"stream","text":"Epoch 2, global step 2925: 'accuracy_score' was not in top 1\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"\n\nEpoch: 3, accuracy_score: 0.66795814037323\n"},{"name":"stderr","output_type":"stream","text":"Epoch 3, global step 3900: 'accuracy_score' reached 0.66796 (best 0.66796), saving model to '/notebooks/model-f3-accuracy_score=0.6680.ckpt' as top 1\n"}]}]}